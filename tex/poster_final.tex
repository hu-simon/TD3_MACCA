\documentclass[portrait,fontscale=0.292,paperheight=48in,paperwidth=36in]{baposter}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{etoolbox}
\AtBeginEnvironment{algorithm}{%
	\setlength{\columnwidth}{\linewidth}%
}
\usepackage{times}
\usepackage{calc}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{booktabs}
\usepackage{ps}

\usepackage{graphicx}
\usepackage{multicol}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{ae}

\setlength{\columnsep}{1.7em}
\setlength{\columnseprule}{0mm}
\newcommand{\compresslist}{%
	\setlength{\itemsep}{1pt}%
	\setlength{\parskip}{0pt}%
	\setlength{\parsep}{0pt}%
}

% The Methods
\newcommand*{\ICIA}{\emph{ICIA}}
\newcommand*{\CoDe}{\emph{CoDe}}
\newcommand*{\LinCoDe}{\emph{LinCoDe}}
\newcommand*{\CoNe}{\emph{CoNe}}
\newcommand*{\CoLiNe}{\emph{CoLiNe}}
\newcommand*{\LinCoLiNe}{\emph{LinCoLiNe}}

\newlength{\maxwidth}
\newcommand{\algalign}[2]% #1 = text to left, #2 = text to right
{\makebox[\maxwidth][r]{$#1{}$}${}#2$}

% inter eye distance
\newcommand*{\ied}{IED}

\DeclareMathOperator{\Clip}{clip}

\begin{document}
\vspace{-7em}
\begin{poster}
	{	
		grid=false, 
		colspacing=0.7em,
		headerColorOne=cyan!20!white!90!black,
		%1, 0.638, 0, 0.447
		%headerColorOne=cyan!1!magenta!0.638!yellow!0!,
		borderColor=cyan!20!white!90!black,	
		textborder=rectangle,
		headerborder=closed,
		headershape=rectangle,
		headershade=plain,
		background=None,
		boxshade=none,
		columns=5,
		bgColorOne=cyan!10!white,
		eyecatcher=false,
		headerheight=0.11\textheight
		%headerheight=0.12\textheight
	}
	{
		\includegraphics[scale=0.2]{voronoi_partition}
		\includegraphics[scale=0.2]{gaussian_simulation_1}
	}
	{\sc\huge\bf Multi-Agent Area Coverage Control using Reinforcement Learning}	
	{\Large\bf Presented by Simon Hu}
	% University logo
	{
		\begin{tabular}{r}
			\raisebox{0em}[0em][0em]{\includegraphics[height=0.03\textheight]{logo}}
		\end{tabular}
	}

\headerbox{Problem Statement}{name=problem_statement,column=0,span=2}
{
	Consider a group of $N$ homogeneous agents moving in a compact environment $\Omega \subset \R^2$ where the dynamics of the agent are given by $\dot{p_i} = g(p_i, u_i)$ where $p_i = (x_i, y_i) \in \R^2$ is the agent position and $u_i = (u_{x_i}, u_{y_i})$ represents the control input. The goal is to find a configuration of agent positions $p = (p_1, p_2, \dots, p_N$ such that the cost index
	\begin{equation*}
		\displaystyle \mathcal{H}(p, t) = \int_{\Omega}{\max\limits_{i=1,\dots,N}{f_i(\,\norm{p_i - q})} \phi(q, t) \dif q}
	\end{equation*}
	is maximized. Applications include harbour control, search and rescue, data collection, and surveillance missions.
}

\headerbox{Voronoi Partitions}{name=voronoi,column=0,span=2,below=problem_statement}
{
	The Voronoi partition of $\Omega$ is given by $\V = \bigcup \V_i$ where each $\V_i$ is given by 
	\begin{equation*}
		\displaystyle \V_i = \left\{ q \in \Omega \: | \: \norm{q - p_i} \leq \norm{q - p_j}, \, \forall j \neq i \right\}.
	\end{equation*}
	The mass $m_{\V_i}$ and center of mass $c_{\V_i}$ of each $V_i$ are given by 
	\begin{equation*}
		\displaystyle m_{\V_i} = \int_{\V_i}{\phi \dif q}, c_{\V_i} = \frac{1}{m_{\V_i}}\int_{\V_i}{q\phi\dif q}
	\end{equation*}
	Then the cost index $\mathcal{H}$ can be rewritten as 
	\begin{equation*}
		\displaystyle \mathcal{H}(p, t) = \sum\limits_{i=1}^{n}{\int_{\V_i}{f_i(\,\norm{p_i - q}) \phi(q, t) \dif q}}.
	\end{equation*}
	\textrm{[Cort\'es et al., 2004]} showed that the optimal partition of $\Omega$ that maximizes $\mathcal{H}$ is the centroidal voronoi partition. So the optimal strategy is to follow the centroids of the computed Voronoi cells. 
	\begin{center}
		\includegraphics[scale=0.299]{fig_ex_2.png}
		\includegraphics[scale=0.299]{fig_ex.png}
		\captionof{figure}{Example of a Voronoi partition, and a centroidal voronoi partition, for $\phi(q,t) = 1$.}
	\end{center}
	%\vspace{-0.6em}
}

\headerbox{Twin-Delayed SAC DDPG (TD3)}{name=td3,column=2,span=3}
{
	\begin{algorithm}[H]
		\caption{Twin-Delayed Actor-Critic DDPG}
		\begin{algorithmic}[1]
			\STATE Initialize critic networks $Q_{\theta_1}, Q_{\theta_2}$ and actor network $\pi_{\phi}$ with random parameters
			$\theta_1, \theta_2, \phi$. 
			\STATE Initialize the target networks $\theta'_1 \leftarrow \theta_1, \theta'_2 \leftarrow \theta_2, \phi' \leftarrow \phi$.
			\STATE Initialize a replay buffer $R$.
			\FOR {$t = 1$ \TO $T$}
				\STATE Select action with exploration noise $a \sim \pi(s) + \epsilon, \epsilon \sim \mathcal{N}(0, \sigma)$ and record the reward $r$ and new state $s'$. 
				\STATE Store the tuple $(s, a, r, s')$ into $R$. 
				\STATE Sample minibatches of $N$ transitions $(s, a, r, s')$ from $R$.
				\STATE Smooth the target policy according to $\tilde{a} \leftarrow \pi_{\phi'}(s) + \epsilon, \epsilon \sim \Clip(\mathcal{N}(0, \tilde{\sigma}), -c, c)$.
				\STATE Clip the results according to the followng rule $y \leftarrow r + \gamma \min_{i=1,2}Q_{\theta'_i}(s', \tilde{a})$. 
				\STATE Update the critics according to $\theta_i \leftarrow \min_{\theta_i}N^{-1}\sum(y - Q_{\theta_i}(s, a))^2$.
				\IF {$t \mod d$} 
					\STATE Update $\phi$ by the deterministic policy gradient according to the following rule $\nabla_{\phi}J(\phi) = N^{-1}\sum\nabla_a Q_{\theta_1}(s,a)\big|_{a = \pi_{\phi(s)}}\nabla_{\phi}\pi_{\phi(s)}$.
					\STATE Update the target networks according to $\theta'_i \leftarrow \tau\theta_i + (1-\tau)\theta'_i, \:\: \phi' \leftarrow \tau\phi + (1-\tau)\phi'$.
				\ENDIF 
			\ENDFOR
		\end{algorithmic}	
	\end{algorithm}
	\vspace{-0.8em}
	The value function is given by
	\begin{equation*}
		\displaystyle V(e_i(k)) = \sum\limits_{\kappa = k}^{\infty}{e_i^T(\kappa)Qe_i(\kappa) + u_i^T(\kappa)Ru_i(\kappa)}
	\end{equation*}
	The value function and policy $\pi_i = u_i$ are approximated by a neural network with weights $\omega_{c, j}$ and $\omega_{a,j}$, and activation functions $\rho$ and $\sigma$:
	\begin{equation*}
		\displaystyle \widehat{V}_j(e_i(k)) = \omega_{c, j}^T\rho(e_i(k)), \:\: \widehat{u}_j(e_i(k)) = \omega_{a,j}^T\sigma(e_i(k)).
	\end{equation*}
	The loss for the critic and actor networks are given by the MSE loss. 
	%\vspace{0.9em}
	%The TD3 algorithm is an improvement on the SACDDPG algorithm, which is more vanilla, and prevents overestimation of the value function by decoupling the action selection and $Q$-value update. TD3 reduces variance by updating the policy at a lower frequency than the $Q$-function updates. A regularization strategy is introduced by adding a small amount of clipped random Gaussian noise to the selected action and then averages it over minibatches. 
}

\headerbox{Results}{name=results,column=2,span=3,below=td3}
{
	\begin{center}
		\includegraphics[scale=.3]{poster_sim1_tracking}
		\includegraphics[scale=.3]{poster_sim1_final}
		\includegraphics[scale=.3]{poster_sim1_cost_}
		\captionof{figure}{Scenario 1: The position of the agents is tracked (left), with the red dot denoting the starting point. The final configuration of the agents is also shown (center). The cost function $\mathcal{H}$ as a function of the number of frames.}
	\end{center}
	\begin{center}
		\includegraphics[scale=.3]{poster_sim2_tracking}
		\includegraphics[scale=.3]{poster_sim2_final}
		\includegraphics[scale=.3]{poster_sim2_cost_}
		\captionof{figure}{Scenario 2. There is good coverage at the dark blue areas, which is the expected result.}
	\end{center}
	\begin{center}
		\includegraphics[scale=.3]{poster_sim3_tracking}
		\includegraphics[scale=.3]{poster_sim3_final}
		\includegraphics[scale=.3]{poster_sim3_cost_}
		\captionof{figure}{Scenario 3. There is good coverage on the center strip, which is the expected result. However, the cost function is high relative to the number of agents and area to cover.}
	\end{center}
}

\headerbox{Experimental Setup}{name=setup,column=0,span=2,below=voronoi}
{
	The dyanmics are given by $\dot{p}_i = u_i$ with maximum velocity 1 m/s. 20 Agents are deployed in a pentagonal environment, described by $(0,100), (-95,31), (-59,-81), (59,-81), (95,31)$, and three scenarios are considered. In all three scenarios, different combinations of Gaussians place at different locations with varying covariances. The covariances in the $x$ and $y$ directions are assumed to be the same in scenarios 1 and 2, but not in scenario 3.
}

\headerbox{}{name=references,column=0,below=setup,span=2}
{
	\small
	\nocite{*}
	\bibliography{ref}
	\bibliographystyle{ieeetr}
	\vspace{0.3em}
}

\end{poster}

\end{document}